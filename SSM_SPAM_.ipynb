{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kr2nc4w_soo",
        "outputId": "5b832d23-2893-4026-c520-51c1578d42d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.9596412556053812\n",
            "\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98       954\n",
            "           1       1.00      0.72      0.84       161\n",
            "\n",
            "    accuracy                           0.96      1115\n",
            "   macro avg       0.98      0.86      0.91      1115\n",
            "weighted avg       0.96      0.96      0.96      1115\n",
            "\n",
            "\n",
            "SVM Accuracy: 0.9865470852017937\n",
            "\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       954\n",
            "           1       0.99      0.91      0.95       161\n",
            "\n",
            "    accuracy                           0.99      1115\n",
            "   macro avg       0.99      0.96      0.97      1115\n",
            "weighted avg       0.99      0.99      0.99      1115\n",
            "\n",
            "\n",
            "Logistic Regression Accuracy: 0.9721973094170404\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98       954\n",
            "           1       0.99      0.81      0.89       161\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.98      0.91      0.94      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('SMSSpamCollection.csv', encoding='latin-1')\n",
        "\n",
        "# Rename columns\n",
        "data = data.rename(columns={\"v1\": \"target\", \"v2\": \"Email text\"})\n",
        "\n",
        "# Convert labels to binary (0 for 'ham' and 1 for 'spam')\n",
        "data['Target'] = data['Target'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Email Text'], data['Target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_vectorized = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Predict with Naive Bayes\n",
        "nb_pred = nb_classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluate Naive Bayes\n",
        "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
        "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
        "print(\"\\nNaive Bayes Classification Report:\")\n",
        "print(classification_report(y_test, nb_pred))\n",
        "\n",
        "# Train SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Predict with SVM\n",
        "svm_pred = svm_classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluate SVM\n",
        "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
        "print(\"\\nSVM Accuracy:\", svm_accuracy)\n",
        "print(\"\\nSVM Classification Report:\")\n",
        "print(classification_report(y_test, svm_pred))\n",
        "\n",
        "# Train Logistic Regression classifier\n",
        "lr_classifier = LogisticRegression()\n",
        "lr_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Predict with Logistic Regression\n",
        "lr_pred = lr_classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluate Logistic Regression\n",
        "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "print(\"\\nLogistic Regression Accuracy:\", lr_accuracy)\n",
        "print(\"\\nLogistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, lr_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the documents and transform them to TF-IDF feature vectors\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "# Print the feature names (terms)\n",
        "print(\"\\nFeature Names (Terms):\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the shape of the TF-IDF matrix\n",
        "print(\"\\nShape of TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.shape)"
      ],
      "metadata": {
        "id": "h5w48Uuwdmv_",
        "outputId": "f466c90c-892c-4c41-972e-57b1af1aa13c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "\n",
            "Feature Names (Terms):\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "\n",
            "Shape of TF-IDF Matrix:\n",
            "(4, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the documents and transform them to TF-IDF feature vectors\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "# Print the feature names (terms)\n",
        "print(\"\\nFeature Names (Terms):\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the shape of the TF-IDF matrix\n",
        "print(\"\\nShape of TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.shape)"
      ],
      "metadata": {
        "id": "kiE3bWRLyYRW",
        "outputId": "8bb01ffa-8684-4f92-bc93-9d8a37b3eae7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "\n",
            "Feature Names (Terms):\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "\n",
            "Shape of TF-IDF Matrix:\n",
            "(4, 9)\n"
          ]
        }
      ]
    }
  ]
}